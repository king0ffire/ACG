{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1OFkszp0oc-"
   },
   "source": [
    "#  AI Graphics | NYU 2024\n",
    "### Name: Qingxin Meng\n",
    "\n",
    "## Assignment - Deform a source mesh to form a target mesh using 3D loss functions\n",
    "\n",
    "This assignment was tested in my local machine instead of Google Colab. All the discussions are based on the outputs from my notebook execution. Since the pytorch3d optimization involves randomness, there might be a huge difference compared to my notebook if you run it again. Additionally, there are strikethrough lines in some discussions which explain phenomena in outputs from different executions.\n",
    "\n",
    "If you decide to get similar outputs to mine, you should always try \"Restart kernel and run all cells.\" This whole notebook takes 800 seconds to run on a 4070ti GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-h1ji4dWHQ5"
   },
   "source": [
    "## 0. Install and Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7JS4K6d0Jy7"
   },
   "source": [
    "Ensure `torch` and `torchvision` are installed. If `pytorch3d` is not installed, install it using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qkuyhyTeRyM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "need_pytorch3d=False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d=True\n",
    "if need_pytorch3d:\n",
    "    if torch.__version__.startswith((\"2.2.\")) and sys.platform.startswith(\"linux\"):\n",
    "        # We try to install PyTorch3D via a released wheel.\n",
    "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "        version_str=\"\".join([\n",
    "            f\"py3{sys.version_info.minor}_cu\",\n",
    "            torch.version.cuda.replace(\".\",\"\"),\n",
    "            f\"_pyt{pyt_version_str}\"\n",
    "        ])\n",
    "        !pip install fvcore iopath\n",
    "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    else:\n",
    "        # We try to install PyTorch3D from source.\n",
    "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylbZGXYBtuvB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pytorch3d.io import load_obj, save_obj,load_objs_as_meshes\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.utils import ico_sphere,torus\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance,\n",
    "    mesh_edge_loss,\n",
    "    mesh_laplacian_smoothing,\n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "from pytorch3d.transforms import RotateAxisAngle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['savefig.dpi'] = 80\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT1JTXu1WHQ_"
   },
   "source": [
    "## 1. Load an obj file and create a Meshes object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTPzfi8_0Jy9"
   },
   "source": [
    "Download the target 3D model of a dolphin. It will be saved locally as a file called `dolphin.obj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFNkB6nQWZSw"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://dl.fbaipublicfiles.com/pytorch3d/data/dolphin/dolphin.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dz0imH-ltuvS"
   },
   "outputs": [],
   "source": [
    "# Load the dolphin mesh.\n",
    "trg_obj = os.path.join('dolphin.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbyRhI8ituvW"
   },
   "outputs": [],
   "source": [
    "# We read the target 3D model using load_obj\n",
    "verts, faces, aux = load_obj(trg_obj)\n",
    "\n",
    "# verts is a FloatTensor of shape (V, 3) where V is the number of vertices in the mesh\n",
    "# faces is an object which contains the following LongTensors: verts_idx, normals_idx and textures_idx\n",
    "# For this tutorial, normals and textures are ignored.\n",
    "faces_idx = faces.verts_idx.to(device)\n",
    "verts = verts.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "St_f2PKAcGV_"
   },
   "outputs": [],
   "source": [
    "# We initialize the source shape to be a sphere of radius 1\n",
    "src_mesh = ico_sphere(4, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrzlhtbFaXSN"
   },
   "source": [
    "1.1 The source mesh is a sphere of radius 1 centered at (0, 0, 0). To speed up the optimization process, we should scale normalize the target mesh and center it at the origin, making it fit the unit sphere. Do this operations with the `verts`tensor and create a `Meshes` object called \"trg_mesh\" with the normalized vertices and the faces indices.\n",
    "\n",
    "1.2 Visualize the source and target meshes using `Plotly` integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyOFi-7taWq5"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code for 1.1\n",
    "centered_verts = verts-verts.mean()\n",
    "largest_distance=centered_verts.norm(dim=1).max()\n",
    "normalized_verts = centered_verts/largest_distance\n",
    "\n",
    "trg_mesh=Meshes([normalized_verts], [faces_idx])\n",
    "\n",
    "\n",
    "# Code for 1.2\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plotly_submeshes(verts,faces,size):\n",
    "    specs=[[{\"type\": \"mesh3d\"} for i in range(size[1])] for j in range(size[0])]\n",
    "    fig = make_subplots(size[0],size[1],specs=specs)\n",
    "    for j in range(size[0]):\n",
    "        for i in range(size[1]):\n",
    "            fig.add_trace(go.Mesh3d(x=verts[j][i][:,0],y=verts[j][i][:,1],z=verts[j][i][:,2],i=faces[j][i][:,0],j=faces[j][i][:,1],k=faces[j][i][:,2]),row=j+1,col=i+1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "verts1=trg_mesh.verts_packed().cpu()\n",
    "faces1=trg_mesh.faces_packed().cpu()\n",
    "#fig1 = go.Figure(data=[go.Mesh3d(x=verts1[:,0], y=verts1[:,1], z=verts1[:,2],i=faces1[:, 0],j=faces1[:, 1],k=faces1[:, 2])])\n",
    "#fig1.show()\n",
    "verts5=verts.cpu()\n",
    "\n",
    "verts2=src_mesh.verts_packed().cpu()\n",
    "faces2=src_mesh.faces_packed().cpu()\n",
    "#fig2 =go.Figure(data=[go.Mesh3d(x=verts2[:,0], y=verts2[:,1], z=verts2[:,2],i=faces2[:, 0],j=faces2[:, 1],k=faces2[:, 2])])\n",
    "#fig2.show()\n",
    "\n",
    "\n",
    "fig1 = plotly_submeshes([[verts1,verts2]],[[faces1,faces2]],[1,2])\n",
    "fig1.show()\n",
    "fig3 = go.Figure(data=[go.Mesh3d(x=verts1[:,0], y=verts1[:,1], z=verts1[:,2],i=faces1[:, 0],j=faces1[:, 1],k=faces1[:, 2]),go.Mesh3d(x=verts2[:,0], y=verts2[:,1], z=verts2[:,2],i=faces2[:, 0],j=faces2[:, 1],k=faces2[:, 2],opacity=0.5)])\n",
    "fig3.show()\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYWDl4VGWHRK"
   },
   "source": [
    "###  Visualizing point clouds with Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geI4FTtwdxsq"
   },
   "source": [
    "We can also sample points from the surfaces and visualize them using Matplotlib. It can be a useful to have a coarse estimation of the surface during iterations in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "482YycLHWHRL"
   },
   "outputs": [],
   "source": [
    "def plot_pointcloud(mesh, title=\"\"):\n",
    "    # Sample points uniformly from the surface of the mesh.\n",
    "    points = sample_points_from_meshes(mesh, 5000)\n",
    "    x, y, z = points.clone().detach().cpu().squeeze().unbind(1)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter3D(x, z, -y)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('z')\n",
    "    ax.set_zlabel('y')\n",
    "    ax.set_title(title)\n",
    "    ax.view_init(190, 30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86dQA5w4dbcg"
   },
   "source": [
    "1.3 Use the function `plot_pointcloud` to see the initial pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoGcflJ_WHRO"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "##############################################################################\n",
    "# Code for 1.3\n",
    "plot_pointcloud(src_mesh,\"src_mesh\")\n",
    "plot_pointcloud(trg_mesh,\"trg_mesh\")\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uzMiTUSWHRS"
   },
   "source": [
    "## 2. Optimization loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekMWSPBOffI1"
   },
   "source": [
    "Starting from a sphere mesh, we learn the offset to each vertex in the mesh such that the predicted mesh is closer to the target mesh at each optimization step. To achieve this we minimize the distance between the predicted (deformed) and target mesh, defined as the **chamfer distance** between the set of pointclouds resulting from **differentiably sampling points** from their surfaces.\n",
    "\n",
    "However, solely minimizing the chamfer distance between the predicted and the target mesh will lead to a non-smooth shape. We enforce smoothness by adding **shape regularizers** to the objective. Namely, we add:\n",
    "\n",
    "+ `mesh_edge_length`, which minimizes the length of the edges in the predicted mesh.\n",
    "+ `mesh_normal_consistency`, which enforces consistency across the normals of neighboring faces.\n",
    "+ `mesh_laplacian_smoothing`, which is the laplacian regularizer.\n",
    "We will learn to deform the source mesh by offsetting its vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sc-3M17Ltuvh"
   },
   "outputs": [],
   "source": [
    "# The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BtSUfMYtuvl"
   },
   "outputs": [],
   "source": [
    "# The optimizer\n",
    "optimizer = torch.optim.SGD([deform_verts], lr=1.0, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3edwRSHcf_99"
   },
   "source": [
    "OBS: Note that you need the \"trg_mesh\" from 1.1 for the optimization code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAP4W7hIf_99"
   },
   "outputs": [],
   "source": [
    "# Number of optimization steps\n",
    "Niter = 2000\n",
    "# Weight for the chamfer loss\n",
    "w_chamfer = 1.0\n",
    "# Weight for mesh edge loss\n",
    "w_edge = 1.0\n",
    "# Weight for mesh normal consistency\n",
    "w_normal = 0.01\n",
    "# Weight for mesh laplacian smoothing\n",
    "w_laplacian = 0.1\n",
    "# Plot period for the losses\n",
    "plot_period = 250\n",
    "loop = tqdm(range(Niter))\n",
    "\n",
    "chamfer_losses = []\n",
    "laplacian_losses = []\n",
    "edge_losses = []\n",
    "normal_losses = []\n",
    "total_losses=[]\n",
    "%matplotlib inline\n",
    "\n",
    "for i in loop:\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Deform the mesh\n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "    # We sample 5k points from the surface of each mesh\n",
    "    sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "    sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "    # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "    loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "    # and (b) the edge length of the predicted mesh\n",
    "    loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "    # mesh normal consistency\n",
    "    loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "    # mesh laplacian smoothing\n",
    "    loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "    # Weighted sum of the losses\n",
    "    loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "    # Print the losses\n",
    "    loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "    # Save the losses for plotting\n",
    "    chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "    edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "    normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "    laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "    total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "    # Plot mesh\n",
    "    if i % plot_period == 0:\n",
    "        plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "    # Optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGcZsvWBWHRc"
   },
   "source": [
    "## Visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87EjaDxZPWsM"
   },
   "outputs": [],
   "source": [
    "def plot_losses(loss_dict, size=(13,5)):\n",
    "  fig = plt.figure(figsize=size)\n",
    "  ax = fig.gca()\n",
    "  for loss_name, loss_values in loss_dict.items():\n",
    "    ax.plot(loss_values, label=loss_name)\n",
    "  ax.legend(fontsize=\"16\")\n",
    "  ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "  ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "  ax.set_title(\"Loss vs iterations\", fontsize=\"16\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRmLaLz-P9bN"
   },
   "outputs": [],
   "source": [
    "losses = {\"chamfer loss\": chamfer_losses,\n",
    "          \"edge loss\": edge_losses,\n",
    "          \"normal loss\": normal_losses,\n",
    "          \"laplacian loss\": laplacian_losses,\n",
    "          \"total loss\":total_losses}\n",
    "plot_losses(losses, (18, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9vSKErDWHRg"
   },
   "source": [
    "## Save the predicted mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frNTBwhRh3Av"
   },
   "outputs": [],
   "source": [
    "# Fetch the verts and faces of the final predicted mesh\n",
    "final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd4b7fXRh0fx"
   },
   "source": [
    "2.1 Scale normalize back the `final_verts` to the original target size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq6_CP9IiEBm"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code for 2.1\n",
    "# a modification in previous block to remember the size\n",
    "final_verts_nback=final_verts*largest_distance\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krikJzrLtuvw"
   },
   "outputs": [],
   "source": [
    "# Store the predicted mesh using save_obj\n",
    "final_obj = os.path.join('./', 'final_model.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omv3QtjcRH6r"
   },
   "source": [
    "2.2 Visualize the target mesh and the final mesh using `Plotly`. Qualitatively, what do you think of the result?\n",
    "\n",
    "2.3 Take a look at the **Loss vs Iteration** graph, paying attention to how the losses are decreasing. Do you think it would be possible to achieve a reasonable result with less iterations? Is it worth to run with more iterations? Explain you thoughts, then run the optimization loop again making the changes to validate your hypothesis.\n",
    "\n",
    "2.4 The loss function used for this task is a linear combination of four losses. Let `w_chamfer = 1.0` and set the other coefficients to zero. Run the  optimization loop again with this configuration and describe the result.\n",
    "\n",
    "**[EXTRA] E.1 Experiment other coefficients configurations and describe the results.**\n",
    "\n",
    "2.5 Experiment with others optimizers such as `Adam` and `RMSprop` in place of `SGD`. Also, try changing the learning rate (`lr`), and `momentum` when these parameters are applied. What do you observe in terms of speed of convergence and quality of final results?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code for 2.2\n",
    "\n",
    "verts4=final_verts_nback.detach().cpu()\n",
    "faces4=final_faces.cpu()\n",
    "#fig4 =go.Figure(data=[go.Mesh3d(x=verts4[:,0], y=verts4[:,1], z=verts4[:,2],i=faces4[:, 0],j=faces4[:, 1],k=faces4[:, 2])])\n",
    "#fig4=make_subplots(1,2,specs=[[{\"type\": \"mesh3d\"}, {\"type\": \"mesh3d\"}]])\n",
    "#fig4.add_trace(go.Mesh3d(x=verts4[:,0], y=verts4[:,1], z=verts4[:,2],i=faces4[:, 0],j=faces4[:, 1],k=faces4[:, 2]), row=1, col=1)\n",
    "\n",
    "#fig4.add_trace(go.Mesh3d(x=verts5[:,0], y=verts5[:,1], z=verts5[:,2],i=faces1[:, 0],j=faces1[:, 1],k=faces1[:, 2]), row=1, col=2)\n",
    "fig4=plotly_submeshes([[verts4,verts5]],[[faces4,faces1]],[1,2])\n",
    "\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discusstion for 2.2\n",
    "\n",
    "The left figure is the final mesh, and the right one is the target mesh. \n",
    "\n",
    "The tail of the final mesh is not as symmetric as that of the target mesh. ~~Additionally, the left part of the tail from the target mesh has obvious artifacts; these can be easily identified even without comparison.~~ Meanwhile, the belly of the final mesh has prominent lines on each side, in contrast to the smooth belly of the target mesh. Also, the dorsal fin on the final mesh is not sharp and appears to have lost a corner compared to the target mesh.\n",
    "\n",
    "In conclusion, the optimized (final) mesh resembles a dolphin, though some details are lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code for 2.3 part1\n",
    "def plot_losses_log(loss_dict, size=(13,5)):\n",
    "  fig = plt.figure(figsize=size)\n",
    "  ax = fig.gca()\n",
    "  for loss_name, loss_values in loss_dict.items():\n",
    "    ax.plot(loss_values, label=loss_name)\n",
    "  ax.legend(fontsize=\"16\")\n",
    "  ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "  ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "  ax.set_title(\"Loss vs iterations\", fontsize=\"16\")\n",
    "  ax.set_yscale('log') \n",
    "losses_less = {\"chamfer loss\": chamfer_losses[250:],\n",
    "          \"edge loss\": edge_losses[250:],\n",
    "          \"normal loss\": normal_losses[250:],\n",
    "          \"laplacian loss\": laplacian_losses[250:]}\n",
    "plot_losses_log(losses, (18, 10))\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for 2.3 part2\n",
    "# The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "# The optimizer\n",
    "optimizer = torch.optim.SGD([deform_verts], lr=1.0, momentum=0.9)\n",
    "# Number of optimization steps\n",
    "Niter = 4000\n",
    "# Weight for the chamfer loss\n",
    "w_chamfer = 1.0\n",
    "# Weight for mesh edge loss\n",
    "w_edge = 1.0\n",
    "# Weight for mesh normal consistency\n",
    "w_normal = 0.01\n",
    "# Weight for mesh laplacian smoothing\n",
    "w_laplacian = 0.1\n",
    "# Plot period for the losses\n",
    "plot_period = 2000\n",
    "loop = tqdm(range(Niter))\n",
    "\n",
    "chamfer_losses = []\n",
    "laplacian_losses = []\n",
    "edge_losses = []\n",
    "normal_losses = []\n",
    "total_losses=[]\n",
    "%matplotlib inline\n",
    "\n",
    "for i in loop:\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Deform the mesh\n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "    # We sample 5k points from the surface of each mesh\n",
    "    sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "    sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "    # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "    loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "    # and (b) the edge length of the predicted mesh\n",
    "    loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "    # mesh normal consistency\n",
    "    loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "    # mesh laplacian smoothing\n",
    "    loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "    # Weighted sum of the losses\n",
    "    loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "    # Print the losses\n",
    "    loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "    # Save the losses for plotting\n",
    "    chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "    edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "    normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "    laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "    total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "    # Plot mesh\n",
    "    if i % plot_period == 0:\n",
    "        plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "    if i == 2000:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] /= 2\n",
    "\n",
    "    # Optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "final_verts_nback=final_verts*largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for 2.3 part3 (visualize the loss)\n",
    "losses = {\"chamfer loss\": chamfer_losses,\n",
    "          \"edge loss\": edge_losses,\n",
    "          \"normal loss\": normal_losses,\n",
    "          \"laplacian loss\": laplacian_losses,\n",
    "          \"total loss\":total_losses}\n",
    "plot_losses_log(losses, (18, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for 2.3 part 4 (visualize the mesh)\n",
    "verts6=final_verts_nback.detach().cpu()\n",
    "faces6=final_faces.cpu()\n",
    "#fig5=make_subplots(1,3,specs=[[{\"type\": \"mesh3d\"}, {\"type\": \"mesh3d\"}],[{\"type\": \"mesh3d\"}]])\n",
    "#fig5.add_trace(go.Mesh3d(x=verts4[:,0], y=verts4[:,1], z=verts4[:,2],i=faces4[:, 0],j=faces4[:, 1],k=faces4[:, 2]), row=1, col=1)\n",
    "#fig5.add_trace(go.Mesh3d(x=verts6[:,0], y=verts6[:,1], z=verts6[:,2],i=faces6[:, 0],j=faces6[:, 1],k=faces6[:, 2]), row=1, col=2)\n",
    "#fig5.add_trace(go.Mesh3d(x=verts5[:,0], y=verts5[:,1], z=verts5[:,2],i=faces1[:, 0],j=faces1[:, 1],k=faces1[:, 2]), row=1, col=3)\n",
    "fig5=plotly_submeshes([[verts4,verts6,verts5]],[[faces4,faces6,faces1]],[1,3])\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for 2.3\n",
    "\n",
    "Above, to further observe the detaied loss variation, we plot the previous loss figure with the y coordinate scaling in logarithm. At the beginning of optimization process, the total loss and chamfer loss has a very high value and decline rapidly. After 1000 iterations, the total loss converges around 10^-2 scale, however, with oscillation.\n",
    "\n",
    "Even though the final mesh (the blue mesh) has a very similar shape to the target mesh, I think the loss could even converge further. To be specific, after 2000 iterations, the final mesh shape is reasonable to human eye. On the other side, further optimization could achieve a more authentic and detailed shape.\n",
    "\n",
    "Thus, I decided to run the optimization with more iterations and a decaying learning rate. The first figure in mesh visualization shows the results at 2000 iterations, and the second at 4000 iterations (the red mesh). Visually, these figures don't make a difference. However, the loss reached  a lower value (0.002287), which demonstrates our hypothesis that the loss could converge more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for 2.4 part1 (optimization)\n",
    "# The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "# The optimizer\n",
    "optimizer = torch.optim.SGD([deform_verts], lr=1.0, momentum=0.9)\n",
    "# Number of optimization steps\n",
    "Niter = 2000\n",
    "# Weight for the chamfer loss\n",
    "w_chamfer = 1.0\n",
    "# Weight for mesh edge loss\n",
    "w_edge = 0.0\n",
    "# Weight for mesh normal consistency\n",
    "w_normal =  0.0\n",
    "# Weight for mesh laplacian smoothing\n",
    "w_laplacian =  0.0\n",
    "# Plot period for the losses\n",
    "plot_period = 1000\n",
    "loop = tqdm(range(Niter))\n",
    "\n",
    "chamfer_losses = []\n",
    "laplacian_losses = []\n",
    "edge_losses = []\n",
    "normal_losses = []\n",
    "total_losses=[]\n",
    "%matplotlib inline\n",
    "\n",
    "for i in loop:\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Deform the mesh\n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "    # We sample 5k points from the surface of each mesh\n",
    "    sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "    sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "    # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "    loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "    # and (b) the edge length of the predicted mesh\n",
    "    loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "    # mesh normal consistency\n",
    "    loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "    # mesh laplacian smoothing\n",
    "    loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "    # Weighted sum of the losses\n",
    "    loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "    # Print the losses\n",
    "    loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "    # Save the losses for plotting\n",
    "    chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "    edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "    normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "    laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "    total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "    # Plot mesh\n",
    "    if i % plot_period == 0:\n",
    "        plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "    if i == 2000:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] /= 2\n",
    "\n",
    "    # Optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "final_verts_nback=final_verts*largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts7=final_verts_nback.detach().cpu()\n",
    "faces7=final_faces.cpu()\n",
    "\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for 2.4 part 2 (visualization)\n",
    "plotly_submeshes([[verts7,verts5]],[[faces7,faces1]],(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for 2.4\n",
    "\n",
    "The loss converges much more rapidly than expected when optimization is constrained by only one type of loss. \n",
    "\n",
    "However, using only chamfer loss results in a completely unsuccessful mesh fit. The final mesh displays a crude and uneven dolphin shape, with the tips of the fins and the mouth becoming diffuse, pinprick-like shapes. It is difficult to identify the species from the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for E.1\n",
    "def weight_experiment(src_mesh,trg_mesh,chamfer,edge,normal,laplacian,iterations):\n",
    "    # The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "    deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "    # The optimizer\n",
    "    optimizer = torch.optim.SGD([deform_verts], lr=1.0, momentum=0.9)\n",
    "    # Number of optimization steps\n",
    "    Niter = iterations\n",
    "    # Weight for the chamfer loss\n",
    "    w_chamfer = chamfer\n",
    "    # Weight for mesh edge loss\n",
    "    w_edge = edge\n",
    "    # Weight for mesh normal consistency\n",
    "    w_normal = normal\n",
    "    # Weight for mesh laplacian smoothing\n",
    "    w_laplacian = laplacian\n",
    "    # Plot period for the losses\n",
    "    plot_period = 1000\n",
    "    loop = tqdm(range(Niter))\n",
    "\n",
    "    chamfer_losses = []\n",
    "    laplacian_losses = []\n",
    "    edge_losses = []\n",
    "    normal_losses = []\n",
    "    total_losses = []\n",
    "    %matplotlib inline\n",
    "\n",
    "    for i in loop:\n",
    "        # Initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Deform the mesh\n",
    "        new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "        # We sample 5k points from the surface of each mesh\n",
    "        sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "        sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "        # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "        loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "        # and (b) the edge length of the predicted mesh\n",
    "        loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "        # mesh normal consistency\n",
    "        loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "        # mesh laplacian smoothing\n",
    "        loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "        # Weighted sum of the losses\n",
    "        loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "        # Print the losses\n",
    "        loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "        # Save the losses for plotting\n",
    "        chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "        edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "        normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "        laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "        total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "        # Plot mesh\n",
    "        if i % plot_period == 0:\n",
    "            plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "        if i == 2000:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "\n",
    "        # Optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "    return final_verts, final_faces\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for E.1 part 1 (edge length weight)\n",
    "final_verts,final_faces = weight_experiment(src_mesh,trg_mesh,1.0,0.5,0.01,0.1,2000)\n",
    "\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts8 = final_verts_nback.detach().cpu()\n",
    "faces8 = final_faces.cpu()\n",
    "\n",
    "final_verts,final_faces =weight_experiment(src_mesh,trg_mesh,1.0,2.0,0.01,0.1,2000)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts9 = final_verts_nback.detach().cpu()\n",
    "faces9 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for E.1 part 1 (visualization)\n",
    "plotly_submeshes([[verts8,verts9,verts5]],[[faces8,faces9,faces1]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for E.1 part 1 (experiment on edge length weight)\n",
    "\n",
    "Above, the first figure (the blue mesh) represents the mesh optimized with an edge length weight of 0.5, the second with a weight of 2.0, and the third is the target mesh.\n",
    "\n",
    "We observed that the first mesh is not smooth enough to be identified as a dolphin, suffering from stretching artifacts in mouth part and at the tip of tail. This indicates that the low edge length loss allows longer egdes exists, and further forms those streching artifacts.\n",
    "\n",
    "For the second mesh (the red mesh), it almost achieves to be a good shape of a dolphin. Nevertheless, the left part of the dolphin has a little artifacts, failing to maintain the overall smooth of the optimizied mesh. We can infer that the spike shape artifacts is caused by the high edge length loss which enforcing shorter edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for E.1 part 2 (normal consistency weight)\n",
    "final_verts,final_faces = weight_experiment(src_mesh,trg_mesh,1.0,1.0,0.02,0.1,2000)\n",
    " \n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts10 = final_verts_nback.detach().cpu()\n",
    "faces10 = final_faces.cpu()\n",
    "\n",
    "final_verts,final_faces =weight_experiment(src_mesh,trg_mesh,1.0,1.0,0.005,0.1,2000)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts11 = final_verts_nback.detach().cpu()\n",
    "faces11 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for E.1 part 2 (visualization)\n",
    "plotly_submeshes([[verts10,verts11,verts5]],[[faces10,faces11,faces1]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for E.1 part 2 (experiment on normal consistency weight)\n",
    "\n",
    "~~We know that normal consistency is a local smooth constraint, and we expect that larger weight of normal consistency makes the mesh more smooth while lower has opposite effect. However the results of experienment are not as expected. Both optimized meshes characterize obvious artifacts on the tail part. We think that the first optimized mesh with larger weight of normal consistency needs more iteration to make sure conversion. Thus we decided to optimize with the same configuration again except through 4000 iterations. The optimization is shown below.~~\n",
    "\n",
    "Note: The discussion above is based on another optimization process. However, with some randomness, 2000 iterations seems to be enough for normal conssitency weight loss experiment, you can check the discussion in following discussion section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for E.1 part 2 (normal consistency weight, 4000 iterations)\n",
    "final_verts,final_faces = weight_experiment(src_mesh,trg_mesh,1.0,1.0,0.02,0.1,4000)\n",
    " \n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts12 = final_verts_nback.detach().cpu()\n",
    "faces12 = final_faces.cpu()\n",
    "\n",
    "final_verts,final_faces =weight_experiment(src_mesh,trg_mesh,1.0,1.0,0.005,0.1,4000)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts13 = final_verts_nback.detach().cpu()\n",
    "faces13 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for E.1 part 2 (visualization)\n",
    "plotly_submeshes([[verts12,verts13,verts5]],[[faces12,faces13,faces1]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for E.1 part 2 continued (experiment on normal consistency weight)\n",
    "\n",
    "The final meshes are now in the perfect shape of a dolphin. The only difference we find is that the dorsal fin of the first mesh is smoother. This proves that the lower the normal consistency loss, the smoother the optimized mesh is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for E.1 part 3 (laplacian smoothing weight)\n",
    "final_verts,final_faces = weight_experiment(src_mesh,trg_mesh,1.0,1.0,0.01,0.2,2000)\n",
    " \n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts14 = final_verts_nback.detach().cpu()\n",
    "faces14 = final_faces.cpu()\n",
    "\n",
    "final_verts,final_faces =weight_experiment(src_mesh,trg_mesh,1.0,1.0,0.01,0.05,2000)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts15 = final_verts_nback.detach().cpu()\n",
    "faces15 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for E.1 part 3 (visualization)\n",
    "plotly_submeshes([[verts14,verts15,verts5]],[[faces14,faces15,faces1]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for E.1 part 3 (experiment on laplacian smoothing) \n",
    "\n",
    "The first figure is optimized by doubled laplacian smoothing weight while the second by halved weight.\n",
    "\n",
    "The laplacian smoothing is a local smooth factor similar to normal consistency. The first figure is oversmoothed while the second figure seems to have a relatively rough surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for E.1 part 4 (chamfer loss weight)\n",
    "final_verts,final_faces = weight_experiment(src_mesh,trg_mesh,2.0,1.0,0.01,0.1,2000)\n",
    " \n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts16 = final_verts_nback.detach().cpu()\n",
    "faces16 = final_faces.cpu()\n",
    "\n",
    "final_verts,final_faces =weight_experiment(src_mesh,trg_mesh,0.5,1.0,0.01,0.1,2000)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts17 = final_verts_nback.detach().cpu()\n",
    "faces17 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for E.1 part 4 (visualization)\n",
    "plotly_submeshes([[verts16,verts17,verts5]],[[faces16,faces17,faces1]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for E.1 part 4 (experiment on chamfer loss)\n",
    "\n",
    "The chamfer loss is a reconstruction loss that is essential in constructing a dolphin shape. The first figure is generated with doubled chamfer loss weight, and the second with halved chamfer loss weight.\n",
    "\n",
    "With lower chamfer loss, the first figure maintains a better dolphin shape.\n",
    "\n",
    "## Conclusion for E.1\n",
    "\n",
    "Well-designed loss weights are constraints that play an important role in constructing a good and accurate mesh. We have experimented with different configurations by changing a single weight to optimize a new mesh. Changing a single weight without considering other weights creates imbalance among those constraints, and that is the main reason most of these experiments show dolphin meshes with flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# code for 2.5\n",
    "def SGD_opt(src_mesh,trg_mesh,chamfer,edge,normal,laplacian,iterations,lr,momentum):\n",
    "    # The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "    deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "    # The optimizer\n",
    "    optimizer = torch.optim.SGD([deform_verts], lr=lr, momentum=momentum)\n",
    "    # Number of optimization steps\n",
    "    Niter = iterations\n",
    "    # Weight for the chamfer loss\n",
    "    w_chamfer = chamfer\n",
    "    # Weight for mesh edge loss\n",
    "    w_edge = edge\n",
    "    # Weight for mesh normal consistency\n",
    "    w_normal = normal\n",
    "    # Weight for mesh laplacian smoothing\n",
    "    w_laplacian = laplacian\n",
    "    # Plot period for the losses\n",
    "    plot_period = 1000\n",
    "    loop = tqdm(range(Niter))\n",
    "\n",
    "    chamfer_losses = []\n",
    "    laplacian_losses = []\n",
    "    edge_losses = []\n",
    "    normal_losses = []\n",
    "    total_losses = []\n",
    "    %matplotlib inline\n",
    "\n",
    "    for i in loop:\n",
    "        # Initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Deform the mesh\n",
    "        new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "        # We sample 5k points from the surface of each mesh\n",
    "        sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "        sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "        # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "        loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "        # and (b) the edge length of the predicted mesh\n",
    "        loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "        # mesh normal consistency\n",
    "        loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "        # mesh laplacian smoothing\n",
    "        loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "        # Weighted sum of the losses\n",
    "        loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "        # Print the losses\n",
    "        loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "        # Save the losses for plotting\n",
    "        chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "        edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "        normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "        laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "        total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "        # Plot mesh\n",
    "        if i % plot_period == 0:\n",
    "            plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "        if i == 2000:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "\n",
    "        # Optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "    losses = {\"chamfer loss\": chamfer_losses,\n",
    "          \"edge loss\": edge_losses,\n",
    "          \"normal loss\": normal_losses,\n",
    "          \"laplacian loss\": laplacian_losses,\n",
    "          \"total loss\":total_losses}\n",
    "\n",
    "    return final_verts, final_faces,losses\n",
    "    \n",
    "def Adam_opt(src_mesh,trg_mesh,chamfer,edge,normal,laplacian,iterations,lr,betas):\n",
    "    # The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "    deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "    # The optimizer\n",
    "    optimizer = torch.optim.Adam([deform_verts], lr=lr, betas=betas)\n",
    "    # Number of optimization steps\n",
    "    Niter = iterations\n",
    "    # Weight for the chamfer loss\n",
    "    w_chamfer = chamfer\n",
    "    # Weight for mesh edge loss\n",
    "    w_edge = edge\n",
    "    # Weight for mesh normal consistency\n",
    "    w_normal = normal\n",
    "    # Weight for mesh laplacian smoothing\n",
    "    w_laplacian = laplacian\n",
    "    # Plot period for the losses\n",
    "    plot_period = 1000\n",
    "    loop = tqdm(range(Niter))\n",
    "\n",
    "    chamfer_losses = []\n",
    "    laplacian_losses = []\n",
    "    edge_losses = []\n",
    "    normal_losses = []\n",
    "    total_losses = []\n",
    "    %matplotlib inline\n",
    "\n",
    "    for i in loop:\n",
    "        # Initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Deform the mesh\n",
    "        new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "        # We sample 5k points from the surface of each mesh\n",
    "        sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "        sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "        # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "        loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "        # and (b) the edge length of the predicted mesh\n",
    "        loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "        # mesh normal consistency\n",
    "        loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "        # mesh laplacian smoothing\n",
    "        loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "        # Weighted sum of the losses\n",
    "        loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "        # Print the losses\n",
    "        loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "        # Save the losses for plotting\n",
    "        chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "        edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "        normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "        laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "        total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "        # Plot mesh\n",
    "        if i % plot_period == 0:\n",
    "            plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "        if i == 2000:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "\n",
    "        # Optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "    losses = {\"chamfer loss\": chamfer_losses,\n",
    "          \"edge loss\": edge_losses,\n",
    "          \"normal loss\": normal_losses,\n",
    "          \"laplacian loss\": laplacian_losses,\n",
    "          \"total loss\":total_losses}\n",
    "\n",
    "    return final_verts, final_faces,losses\n",
    "\n",
    "def RMSprop_opt(src_mesh,trg_mesh,chamfer,edge,normal,laplacian,iterations,lr,momentum):\n",
    "    # The shape of the deform parameters is equal to the total number of vertices in src_mesh\n",
    "    deform_verts = torch.full(src_mesh.verts_packed().shape, 0.0, device=device, requires_grad=True)\n",
    "    # The optimizer\n",
    "    optimizer = torch.optim.RMSprop([deform_verts], lr=lr, momentum=momentum)\n",
    "    # Number of optimization steps\n",
    "    Niter = iterations\n",
    "    # Weight for the chamfer loss\n",
    "    w_chamfer = chamfer\n",
    "    # Weight for mesh edge loss\n",
    "    w_edge = edge\n",
    "    # Weight for mesh normal consistency\n",
    "    w_normal = normal\n",
    "    # Weight for mesh laplacian smoothing\n",
    "    w_laplacian = laplacian\n",
    "    # Plot period for the losses\n",
    "    plot_period = 1000\n",
    "    loop = tqdm(range(Niter))\n",
    "\n",
    "    chamfer_losses = []\n",
    "    laplacian_losses = []\n",
    "    edge_losses = []\n",
    "    normal_losses = []\n",
    "    total_losses = []\n",
    "    %matplotlib inline\n",
    "\n",
    "    for i in loop:\n",
    "        # Initialize optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Deform the mesh\n",
    "        new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "        # We sample 5k points from the surface of each mesh\n",
    "        sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n",
    "        sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n",
    "\n",
    "        # We compare the two sets of pointclouds by computing (a) the chamfer loss\n",
    "        loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n",
    "\n",
    "        # and (b) the edge length of the predicted mesh\n",
    "        loss_edge = mesh_edge_loss(new_src_mesh)\n",
    "\n",
    "        # mesh normal consistency\n",
    "        loss_normal = mesh_normal_consistency(new_src_mesh)\n",
    "\n",
    "        # mesh laplacian smoothing\n",
    "        loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=\"uniform\")\n",
    "\n",
    "        # Weighted sum of the losses\n",
    "        loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n",
    "\n",
    "        # Print the losses\n",
    "        loop.set_description('total_loss = %.6f' % loss)\n",
    "\n",
    "        # Save the losses for plotting\n",
    "        chamfer_losses.append(float(loss_chamfer.detach().cpu()))\n",
    "        edge_losses.append(float(loss_edge.detach().cpu()))\n",
    "        normal_losses.append(float(loss_normal.detach().cpu()))\n",
    "        laplacian_losses.append(float(loss_laplacian.detach().cpu()))\n",
    "        total_losses.append(float(loss.detach().cpu()))\n",
    "\n",
    "        # Plot mesh\n",
    "        if i % plot_period == 0:\n",
    "            plot_pointcloud(new_src_mesh, title=\"iter: %d\" % i)\n",
    "\n",
    "        if i == 2000:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "\n",
    "        # Optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses = {\"chamfer loss\": chamfer_losses,\n",
    "          \"edge loss\": edge_losses,\n",
    "          \"normal loss\": normal_losses,\n",
    "          \"laplacian loss\": laplacian_losses,\n",
    "          \"total loss\":total_losses}\n",
    "    final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "    return final_verts, final_faces,losses\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# code for 2.5 part 1 (normal config)\n",
    "final_verts, final_faces,losses1=SGD_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,1.0,0.9)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts18 = final_verts_nback.detach().cpu()\n",
    "faces18 = final_faces.cpu()\n",
    "\n",
    "final_verts, final_faces,losses2=Adam_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,1.0,(0.9,0.999))\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts19 = final_verts_nback.detach().cpu()\n",
    "faces19 = final_faces.cpu()\n",
    "\n",
    "final_verts, final_faces,losses3=RMSprop_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,1.0,0.9)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts20 = final_verts_nback.detach().cpu()\n",
    "faces20 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "plot_losses(losses1,(18,10))\n",
    "plot_losses(losses2,(18,10))\n",
    "plot_losses(losses3,(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "plotly_submeshes([[verts18,verts19,verts20]],[[faces18,faces19,faces20]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for 2.5 part 1 (normal config)\n",
    "\n",
    "The first loss figure and the first mesh represent the result from SGD, the second pair represent the result from Adam, and the third pair represent the result from RMSprop. \n",
    "\n",
    "The learning rate of 1.0 seems to be effective only for SGD method, while the Adam and the RMSprop method cannot reconstruct mesh in such configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# code for 2.5 part 2 (learning rate)\n",
    "final_verts, final_faces,losses4=SGD_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,0.1,0.9)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts21 = final_verts_nback.detach().cpu()\n",
    "faces21 = final_faces.cpu()\n",
    "\n",
    "final_verts, final_faces,losses5=Adam_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,0.001,(0.9,0.999))\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts22 = final_verts_nback.detach().cpu()\n",
    "faces22 = final_faces.cpu()\n",
    "\n",
    "final_verts, final_faces,losses6=RMSprop_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,0.0001,0.9)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts23 = final_verts_nback.detach().cpu()\n",
    "faces23 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "plot_losses_log(losses4,(18,10))\n",
    "plot_losses_log(losses5,(18,10))\n",
    "plot_losses_log(losses6,(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u",
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "plotly_submeshes([[verts21,verts22,verts23]],[[faces21,faces22,faces23]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for 2.5 part 2 (learning rate)\n",
    "\n",
    "Above, to make sure the output mesh is not non-sense, we have tried different learning rates to for these 3 methods. We have shown 1 setting per method. \n",
    "\n",
    "The first pair, SGD, has the largest learning rate of 0.1. However, this learning rate is not enough for the mesh to converge to a dolphin shape. So, the SGD converges slowest. \n",
    "\n",
    "The second pair, Adam, has the learning rate of 0.001. The final mesh from this setting presents some detail of a dolphin. By observing the loss curve, the loss does not converge to its best situtation. And more iterations or better learning rate would improve the result. Compared to the first and the third method, the Adam converges in a moderate speed.\n",
    "\n",
    "The third pair, RMSprop, has the smallest learning rate of 0.0001. It has the best reconstruction shape among these 3 methods. And its loss has converged below 10-3 scale which makes it lowest loss among these 3 experiment. We can conclude that RMSprop method has the fastest converging speed and the best loss, in 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "# code for 2.5 part 3 (momentum)\n",
    "final_verts, final_faces,losses7=SGD_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,0.1,0.5)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts24 = final_verts_nback.detach().cpu()\n",
    "faces24 = final_faces.cpu()\n",
    "\n",
    "final_verts, final_faces,losses8=Adam_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,0.001,(0.5,0.999))\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts25 = final_verts_nback.detach().cpu()\n",
    "faces25 = final_faces.cpu()\n",
    "\n",
    "final_verts, final_faces,losses9=RMSprop_opt(src_mesh,trg_mesh,1.0,1.0,0.01,0.1,2000,0.0005,0.5)\n",
    "final_verts_nback = final_verts * largest_distance\n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts26 = final_verts_nback.detach().cpu()\n",
    "faces26 = final_faces.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "plot_losses_log(losses7,(18,10))\n",
    "plot_losses_log(losses8,(18,10))\n",
    "plot_losses_log(losses9,(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "outputs": [],
   "source": [
    "plotly_submeshes([[verts24,verts25,verts26]],[[faces24,faces25,faces26]],(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4kRPbgftz1u"
   },
   "source": [
    "## Discussion for 2.5 part 3 (momentum)\n",
    "\n",
    "Above, we experiment these 3 methods with lower mementum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S13xm7HecK1h"
   },
   "source": [
    "# 3. Experimenting with Other Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt9CcWX2uGhP"
   },
   "outputs": [],
   "source": [
    "# download the mug model - Mug by Microsoft is licensed under Creative Commons Attribution\n",
    "# originally found at https://sketchfab.com/3d-models/mug-17c4808537f1448590378b3643c6da72\n",
    "!wget -nc https://raw.githubusercontent.com/hallpaz/3dsystems21/main/data/mug.obj\n",
    "!wget -nc https://github.com/alecjacobson/common-3d-test-models/blob/master/data/stanford-bunny.obj\n",
    "#!wget -nc https://github.com/alecjacobson/common-3d-test-models/blob/master/data/beetle.obj\n",
    "!wget -nc https://raw.githubusercontent.com/king0ffire/data/main/ring.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AjX0f9JcM73"
   },
   "source": [
    "3.1 Is it possible to deform any mesh into another? Explain your thoughts.\n",
    "\n",
    "3.2 Run the experiment again and try to deform a sphere into a mug.\n",
    "\n",
    "3.3 Change the `src_mesh` to a `torus`. You can import the torus primitive from `pytorch3d.utils`. Now, try to deform the torus into a mug.\n",
    "\n",
    "\n",
    "**[Extra] E.2 Run the experiments again using other shapes (either find and download shapes from the internet or create your own models).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AjX0f9JcM73"
   },
   "source": [
    "## Answer for 3.1\n",
    "\n",
    "It is not possible to deform any mesh into another. However, we can add constraints to make this statement true. Topologically speaking, if two meshes share the same topology, they can be deformed into each other. That is to say, only homeomorphic meshes can be deformed into each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd6zc5Fetwcy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Code for 3.2\n",
    "verts,faces,_=load_obj(\"mug.obj\")\n",
    "faces_idx = faces.verts_idx.to(device)\n",
    "verts = verts.to(device)\n",
    "centered_verts = verts-verts.mean(dim=0)#\n",
    "largest_distance=centered_verts.norm(dim=1).max()\n",
    "normalized_verts = centered_verts/largest_distance\n",
    "mug_mesh=Meshes([normalized_verts], [faces_idx])\n",
    "verts31=normalized_verts.cpu()\n",
    "faces31=faces_idx.cpu()\n",
    "\n",
    "fig6 = go.Figure(data=[go.Mesh3d(x=verts2[:,0], y=verts2[:,1], z=verts2[:,2],i=faces2[:, 0],j=faces2[:, 1],k=faces2[:, 2],opacity=0.5),go.Mesh3d(x=verts31[:,0], y=verts31[:,1], z=verts31[:,2],i=faces31[:, 0],j=faces31[:, 1],k=faces31[:, 2],opacity=0.5)])\n",
    "fig6.show()\n",
    "\n",
    "final_verts, final_faces,losses10=RMSprop_opt(src_mesh,mug_mesh,1.0,1.0,0.01,0.1,2000,0.0001,0.9)\n",
    "final_verts_nback = final_verts \n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts27 = final_verts_nback.detach().cpu()\n",
    "faces27 = final_faces.cpu()\n",
    "verts28 = mug_mesh.verts_packed().cpu()\n",
    "faces28=mug_mesh.faces_packed().cpu()\n",
    "\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd6zc5Fetwcy"
   },
   "outputs": [],
   "source": [
    "plotly_submeshes([[verts27,verts28]],[[faces27,faces28]],(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for 3.3\n",
    "verts,faces,_=load_obj(\"mug.obj\")\n",
    "faces_idx = faces.verts_idx.to(device)\n",
    "verts = verts.to(device)\n",
    "centered_verts = verts-verts.mean(dim=0)#\n",
    "largest_distance=centered_verts.norm(dim=1).max()\n",
    "normalized_verts = centered_verts/largest_distance+torch.tensor([-0.4,0.0,0.0],device=device)\n",
    "mug_mesh=Meshes([normalized_verts], [faces_idx])\n",
    "verts31=normalized_verts.cpu()\n",
    "faces31=faces_idx.cpu()\n",
    "\n",
    "torus_mesh=torus(0.65,0.7,100,100,device)\n",
    "\n",
    "verts30=torus_mesh.verts_packed().cpu()\n",
    "faces30=torus_mesh.faces_packed().cpu()\n",
    "\n",
    "fig7 = go.Figure(data=[go.Mesh3d(x=verts30[:,0], y=verts30[:,1], z=verts30[:,2],i=faces30[:, 0],j=faces30[:, 1],k=faces30[:, 2],opacity=0.5),go.Mesh3d(x=verts31[:,0], y=verts31[:,1], z=verts31[:,2],i=faces31[:, 0],j=faces31[:, 1],k=faces31[:, 2],opacity=0.5)])\n",
    "fig7.show()\n",
    "\n",
    "final_verts, final_faces,losses11=RMSprop_opt(torus_mesh,mug_mesh,1.0,0.5,0.01,0.1,2000,0.0001,0.9)\n",
    "final_verts_nback = final_verts \n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts29 = final_verts_nback.detach().cpu()\n",
    "faces29 = final_faces.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_submeshes([[verts29,verts28]],[[faces29,faces28]],(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for E.2\n",
    "verts,faces,_=load_obj(\"ring.obj\")\n",
    "faces_idx = faces.verts_idx.to(device)\n",
    "verts = verts.to(device)\n",
    "centered_verts = verts-verts.mean(dim=0)\n",
    "rotate_transform = RotateAxisAngle(angle=90, axis=\"X\").to(device)\n",
    "centered_verts = rotate_transform.transform_points(centered_verts)\n",
    "largest_distance=centered_verts.norm(dim=1).max()\n",
    "normalized_verts = centered_verts/largest_distance+torch.tensor([0.0,-0.3,0.0],device=device)\n",
    "normalized_verts = normalized_verts*1.5\n",
    "\n",
    "ring_mesh=Meshes([normalized_verts], [faces_idx])\n",
    "verts32=normalized_verts.cpu()  # 32 is normalized ring\n",
    "faces32=faces_idx.cpu()\n",
    "\n",
    "torus_mesh=torus(0.6,1.0,100,100,device)\n",
    "\n",
    "verts30=torus_mesh.verts_packed().cpu()\n",
    "faces30=torus_mesh.faces_packed().cpu()\n",
    "\n",
    "fig8 = go.Figure(data=[go.Mesh3d(x=verts30[:,0], y=verts30[:,1], z=verts30[:,2],i=faces30[:, 0],j=faces30[:, 1],k=faces30[:, 2],opacity=0.5),go.Mesh3d(x=verts32[:,0], y=verts32[:,1], z=verts32[:,2],i=faces32[:, 0],j=faces32[:, 1],k=faces32[:, 2],opacity=0.5)])\n",
    "fig8.show()\n",
    "\n",
    "final_verts, final_faces,losses12=RMSprop_opt(torus_mesh,ring_mesh,1.0,1.0,0.01,0.1,2000,0.0001,0.9)\n",
    "final_verts_nback = final_verts \n",
    "#final_obj = os.path.join('./', 'final_model3.obj')\n",
    "#save_obj(final_obj, final_verts_nback, final_faces)\n",
    "verts33 = final_verts_nback.detach().cpu()\n",
    "faces33 = final_faces.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_submeshes([[verts33,verts32]],[[faces33,faces32]],(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion for 3.1-E.2\n",
    "\n",
    "The results in Section 3.2-E.2 demonstrate our thoughts from Section 3.1. \n",
    "\n",
    "In the 'Code for 3.2' section, we tried to deform a sphere into a mug. The optimized mesh resembles a basic mug shape. However, the handle is not formed since no hole is observed inside the handle. \n",
    "\n",
    "In the 'Code for 3.3' section, we tried to deform a torus into a mug. Although the torus and mug share the same topology, with one hole, the optimized mesh has formed a handle with a hole inside it. However, since the mug is double-walled, it is very hard for the torus to stretch to fit the cup, resulting in a very uneven appearance. We have tried different configurations including the torus sides, learning rate, and loss weights, but it remains a very challenging task to perfectly deform the torus into a mug. Thus, we decided to deform the torus into other simpler topologically similar meshes with one hole. \n",
    "\n",
    "In the 'Code for E.2' section, we deformed the torus into a ring with a sphere-shaped gemstone on it. The resulting mesh (the blue mesh) has a very similar shape to the original ring (the red mesh). This successful deformation shows that homeomorphic meshes can be deformed into each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TVFzxPT_tWL"
   },
   "source": [
    "# 4. Food for Thoughts\n",
    "\n",
    "Let's say you have a big set Ω of arbitrary meshes, and 1 mesh or a small set Δ of meshes, for example from the same class, that you are instered in. How would you solve the problem of fitting meshes from Ω to meshes in Δ?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
